AUFGABE 1)

1)
Ein Batch-Queuing System hat die Aufgabe die
Anfragen mehrerer Benutzer die zum Teil sequenzielle
aber auch Parrelle Aufgaben an einen Cluster haben
effektiv zu verteilen. 
Es handelt sich hierbei um eine Stapelverarbeitung
mit effizientem Workload.

2)
SLURM, OAR, Condor, Univa Grid Engine, 
Maui Cluster Scheduler, Moab Cluster Suite,

3)
Bei sbatch handelt es sich um die batch komponente
von SLURM.
Diese verteilt die Programme auf die verschiedenen
Knoten und teilt rechenzeit zu.

4)
Auf dem Cluster wird SLURM verwendet

5)
Die aktuellen jobs kann man mit squeue anzeigen lassen.
Beziehungsweise mit sacct um mehr infoueber seine J
und andere Jobs zu erhalten. Weitere Infos koenner ueber
smap erhalten werden.

6)
sview fast alle wichtige befehle von SLURM zusammen.
Da diese in eine GUI dagestellt werden, ist es 
einfacher als ueber ein Terminal.
Des Weiteren bildet sview viele Processe von SLURM
auch Grafisch ab, so dass sie schneller zu ueberblicken
sind.

7)
Ja mittels scancel PID
wobei PID die aktuelle Prozessnummer des Jobs ist.

8)
Ja es koennen mehrere Jobs auf einem Cluster laufen.
Allerdings darf die Aktuellen Jobs nicht alle 
Prozessoren aller Nodes allokiern und/oder
exklusiv laufen damit auch andere Jobs die Knoten 
benutzen koennen.

9)
Mittels dem oben beschrieben squeue koennen 
detailierte Informationen ueber einen oder
mehrere Jobs ausgegeben werden.

10)
Folgende verfahren sind laut 
https://computing.llnl.gov/linux/slurm/documentation.html
moeglich:

-Gang Scheduling
-Generic Resource Scheduling
-buildin (FIFO (First In First Out / entspricht der default Einstellung))
-backfill (Hoeher priorisierte Jobs werden schneller abgearbeitet als weniger priorisierte)


Sowie einige externe Scheduler:

-Maui Scheduler
-Moab Cluster Suite
-Platform LSF 

Auf dem Cluster wird backfill benutzt (zu sehen mit sview)

11)
Mittels 
lloc --nodelist=west7 -N 1
kann der Knoten names west7 allokiert werden.
Um hostname auszufuehren wird:
srun hostname 
benutzt.

12)
Das Timeout auf dem Cluster liegt bei 6 Stunden.

13)
Ab der Version 2 von SLURM kann mittels
des Plugins Multifactor die Priority festgelegt werden.
Dieses Plugin wird auch vom Cluster benutzt.
Da
scontrol view config | grep priority

folgendes ausgibt:
PriorityType            = priority/multifactor

Nachschauen kann man die Priority wie folgt:
sprio 
listet alle Prioritaeten aller jobs.

Da die Prioritaeten ueber einen Fair Faktor 
verteilt werden kann ein Benutzer selbst die
Prioritaet nicht festlegen.

14)
Es sind 5 unterschiedliche Partitionen eingerichtet.
Dies kann man mittels sinfo einsehen.
Hier erhaelt man:

PARTITION AVAIL  TIMELIMIT  NODES  STATE NODELIST
cluster*     up    6:00:00     10   idle west[1-10]
compute      up    6:00:00     10   idle amd[1-5],nehalem[1-5]
abu          up    6:00:00      1  down* abu4
abu          up    6:00:00      4   idle abu[1-3,5]
magny        up   12:00:00      1   idle magny1

Dies kann man mittels 
scontorl view partition ueberpruefen.

Somit sind die Partitionen 
cluster, 
compute, 
abu,
magny


Eine andere partition kann mittels sbatch durch
die option -p, --partition=<partition_names>
angeben werden.


AUFGABE 2)
1)
Siehe timescript

2)
siehe job_script

3)
siehe job_script.out und timescript.out

4)
1. Frage:
Die Knoten fuehren die zugewiesenen Jobs
nicht nach einer bestimmten reihen folge aus.
Da kein Muster in den zeiten zu erkennen sind.

2. Frage:
Dies funktioniert nicht, da das timescript
auf jedem Cluster seperat laeuft wird
durch die Ausgabe von timescript auf jedem Node
eine Datei timescript.out erzeugt.

Daher muss die resultate mittels des job scripts
gesammelt und in eine Datei auf dem login knoten 
geschrieben werden.
